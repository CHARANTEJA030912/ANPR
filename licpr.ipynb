{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2134c3de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torch import nn\n",
    "import pytesseract\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5df9ef6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Dataset Paths\n",
    "TRAIN1_PATH = r\"C:\\Users\\chara\\Downloads\\licpr\\Licplatesdetection_train1\"  # Vehicle images with bounding box annotations\n",
    "TRAIN1_ANNOTATIONS = r\"C:\\Users\\chara\\Downloads\\licpr\\Licplatesdetection_train1_annotations.csv\"  # Annotations for Training Set 1\n",
    "\n",
    "TRAIN2_PATH = r\"C:\\Users\\chara\\Downloads\\licpr\\Licplatesrecognition_train2\"  # License plate images with text annotations\n",
    "TRAIN2_ANNOTATIONS = r\"C:\\Users\\chara\\Downloads\\licpr\\Licplatesrecognition_train2_annotations.csv\"  # Text annotations for Training Set 2\n",
    "\n",
    "TEST_PATH = r\"C:\\Users\\chara\\Downloads\\licpr\\test\" # Test set images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9c7a7ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Training Set 1\n",
    "class LicensePlateDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, transform=None):\n",
    "        \"\"\"\n",
    "        image_dir: Directory containing the images.\n",
    "        annotations_file: CSV file with bounding box annotations.\n",
    "        transform: Data augmentation or transformations.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and annotations\n",
    "        img_path = os.path.join(self.image_dir, self.annotations.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        boxes = self.annotations.iloc[idx, 1:5].values.astype('float32').tolist()\n",
    "        labels = torch.ones((1,), dtype=torch.int64)  # Assuming single class (license plate)\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        target = {\n",
    "            \"boxes\": torch.tensor([boxes]),\n",
    "            \"labels\": labels,\n",
    "        }\n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "488fb33e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\chara\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=FasterRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=FasterRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Build the License Plate Detection Model\n",
    "def build_detection_model():\n",
    "    \"\"\"\n",
    "    Creates a Faster R-CNN model pre-trained on the COCO dataset.\n",
    "    \"\"\"\n",
    "    model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    # Modify the classifier to detect only one class (license plate)\n",
    "    num_classes = 2  # Background + License Plate\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = nn.Linear(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "# Initialize the model\n",
    "detection_model = build_detection_model()\n",
    "detection_model = detection_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ecf8da7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Dataset for Training Set 2 (License Plate OCR)\n",
    "class LicensePlateOCRDataset(Dataset):\n",
    "    def __init__(self, image_dir, annotations_file, transform=None):\n",
    "        \"\"\"\n",
    "        image_dir: Directory containing cropped license plate images.\n",
    "        annotations_file: CSV file with license plate text annotations.\n",
    "        transform: Data augmentation or transformations.\n",
    "        \"\"\"\n",
    "        self.image_dir = image_dir\n",
    "        self.annotations = pd.read_csv(annotations_file)\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.annotations)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Load image and corresponding text\n",
    "        img_path = os.path.join(self.image_dir, self.annotations.iloc[idx, 0])\n",
    "        image = cv2.imread(img_path)\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        label = self.annotations.iloc[idx, 1]\n",
    "\n",
    "        # Apply transformations\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4803bde2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\chara\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "# Build OCR Model using Pre-trained ResNet\n",
    "class LicensePlateOCR(nn.Module):\n",
    "    def __init__(self, num_classes=36):  # 26 letters + 10 digits\n",
    "        super(LicensePlateOCR, self).__init__()\n",
    "        self.backbone = models.resnet18(pretrained=True)\n",
    "        self.backbone.fc = nn.Linear(self.backbone.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "# Initialize the OCR model\n",
    "ocr_model = LicensePlateOCR(num_classes=36)  # Adjust for alphanumeric output\n",
    "ocr_model = ocr_model.to('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0a618cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Function for Detection Model\n",
    "def train_detection_model(model, dataloader, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, targets in dataloader:\n",
    "            images = list(img.to('cpu') for img in images)\n",
    "            targets = [{k: v.to('cpu') for k, v in t.items()} for t in targets]\n",
    "\n",
    "            loss_dict = model(images, targets)\n",
    "            losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            losses.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += losses.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")\n",
    "\n",
    "# Training Function for OCR Model\n",
    "def train_ocr_model(model, dataloader, criterion, optimizer, num_epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0\n",
    "        for images, labels in dataloader:\n",
    "            images = images.to('cpu')\n",
    "            labels = labels.to('cpu')\n",
    "\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9ee76ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Inference Pipeline with Results Storage\n",
    "def detect_and_recognize(image_paths, detection_model, ocr_model, output_excel_path):\n",
    "    \"\"\"\n",
    "    Processes a list of image paths, detects license plates, recognizes characters, \n",
    "    and saves results in an Excel file.\n",
    "\n",
    "    image_paths: List of paths to test images.\n",
    "    detection_model: Pre-trained license plate detection model.\n",
    "    ocr_model: Pre-trained OCR model.\n",
    "    output_excel_path: Path to save the Excel file.\n",
    "    \"\"\"\n",
    "    results = []  # To store results in a list\n",
    "\n",
    "    for image_path in image_paths:\n",
    "        # Read the image\n",
    "        image = cv2.imread(image_path)\n",
    "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        transform = transforms.ToTensor()\n",
    "        image_tensor = transform(image_rgb).unsqueeze(0).to('cpu')\n",
    "\n",
    "        # Step 1: Detect License Plate\n",
    "        detection_model.eval()\n",
    "        with torch.no_grad():\n",
    "            predictions = detection_model(image_tensor)\n",
    "\n",
    "        # Extract bounding box if available\n",
    "        if len(predictions[0]['boxes']) > 0:\n",
    "            box = predictions[0]['boxes'][0].cpu().numpy().astype(int)\n",
    "            cropped = image_rgb[box[1]:box[3], box[0]:box[2]]\n",
    "\n",
    "            # Step 2: OCR Recognition\n",
    "            ocr_model.eval()\n",
    "            transform = transforms.Compose([\n",
    "                transforms.ToPILImage(),\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor()\n",
    "            ])\n",
    "            cropped_tensor = transform(cropped).unsqueeze(0).to('cpu')\n",
    "\n",
    "            with torch.no_grad():\n",
    "                output = ocr_model(cropped_tensor)\n",
    "                predicted_text = \"\".join(chr(c + ord('A')) for c in output.argmax(dim=1).cpu().numpy())\n",
    "        else:\n",
    "            predicted_text = \"License plate not detected\"\n",
    "\n",
    "        # Append the result (image name and detected text)\n",
    "        results.append({\n",
    "            \"Image Name\": os.path.basename(image_path),\n",
    "            \"Detected Text\": predicted_text\n",
    "        })\n",
    "\n",
    "    # Convert results to a DataFrame and save as Excel\n",
    "    df_results = pd.DataFrame(results)\n",
    "    df_results.to_excel(output_excel_path, index=False)\n",
    "    print(f\"Results saved to {output_excel_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "344eed0b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 8\u001b[0m\n\u001b[0;32m      5\u001b[0m output_excel_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlicense_plate_results.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m detect_and_recognize(test_image_paths, detection_model, ocr_model, output_excel_path)\n",
      "Cell \u001b[1;32mIn[9], line 26\u001b[0m, in \u001b[0;36mdetect_and_recognize\u001b[1;34m(image_paths, detection_model, ocr_model, output_excel_path)\u001b[0m\n\u001b[0;32m     24\u001b[0m detection_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m     25\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 26\u001b[0m     predictions \u001b[38;5;241m=\u001b[39m detection_model(image_tensor)\n\u001b[0;32m     28\u001b[0m \u001b[38;5;66;03m# Extract bounding box if available\u001b[39;00m\n\u001b[0;32m     29\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(predictions[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mboxes\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\detection\\generalized_rcnn.py:105\u001b[0m, in \u001b[0;36mGeneralizedRCNN.forward\u001b[1;34m(self, images, targets)\u001b[0m\n\u001b[0;32m    103\u001b[0m     features \u001b[38;5;241m=\u001b[39m OrderedDict([(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m0\u001b[39m\u001b[38;5;124m\"\u001b[39m, features)])\n\u001b[0;32m    104\u001b[0m proposals, proposal_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrpn(images, features, targets)\n\u001b[1;32m--> 105\u001b[0m detections, detector_losses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroi_heads(features, proposals, images\u001b[38;5;241m.\u001b[39mimage_sizes, targets)\n\u001b[0;32m    106\u001b[0m detections \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform\u001b[38;5;241m.\u001b[39mpostprocess(detections, images\u001b[38;5;241m.\u001b[39mimage_sizes, original_image_sizes)  \u001b[38;5;66;03m# type: ignore[operator]\u001b[39;00m\n\u001b[0;32m    108\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\torchvision\\models\\detection\\roi_heads.py:763\u001b[0m, in \u001b[0;36mRoIHeads.forward\u001b[1;34m(self, features, proposals, image_shapes, targets)\u001b[0m\n\u001b[0;32m    761\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_roi_pool(features, proposals, image_shapes)\n\u001b[0;32m    762\u001b[0m box_features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_head(box_features)\n\u001b[1;32m--> 763\u001b[0m class_logits, box_regression \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbox_predictor(box_features)\n\u001b[0;32m    765\u001b[0m result: List[Dict[\u001b[38;5;28mstr\u001b[39m, torch\u001b[38;5;241m.\u001b[39mTensor]] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    766\u001b[0m losses \u001b[38;5;241m=\u001b[39m {}\n",
      "\u001b[1;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# List of test image paths\n",
    "test_image_paths = [os.path.join(TEST_PATH, img) for img in os.listdir(TEST_PATH) if img.endswith(('.png', '.jpg', '.jpeg'))]\n",
    "\n",
    "# Path to save the Excel file\n",
    "output_excel_path = \"license_plate_results.xlsx\"\n",
    "\n",
    "# Run the pipeline\n",
    "detect_and_recognize(test_image_paths, detection_model, ocr_model, output_excel_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1180c10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
